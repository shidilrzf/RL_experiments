{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import copy\n",
    "\n",
    "\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import  DataLoader, Dataset\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import DQN_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cuda = True\n",
    "if torch.cuda.is_available() and not no_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# env_name = 'maze2d-medium-v1'\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "env = gym.make(env_name)\n",
    "is_atari = gym.envs.registry.spec(env_name).entry_point == 'gym.envs.atari:AtariEnv'\n",
    "\n",
    "print(is_atari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim:4\n",
      "action dim:2\n",
      "state shape:(4,)\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "# max_action = float(env.action_space.)\n",
    "\n",
    "print('state dim:{}'.format(state_dim))\n",
    "print('action dim:{}'.format(action_dim))\n",
    "print('state shape:{}'.format(env.observation_space.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, batch_size, buffer_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_size = int(buffer_size)\n",
    "        self.device = device\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.crt_size = 0\n",
    "\n",
    "        self.state = np.zeros((self.max_size, state_dim))\n",
    "        self.action = np.zeros((self.max_size, action_dim))\n",
    "        self.next_state = np.array(self.state)\n",
    "        self.reward = np.zeros((self.max_size, 1))\n",
    "        self.not_done = np.zeros((self.max_size, 1))\n",
    "\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.crt_size = min(self.crt_size + 1, self.max_size)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        ind = np.random.randint(0, self.crt_size, size=self.batch_size)\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind, :]).to(self.device),\n",
    "            torch.LongTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind, :]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )\n",
    "\n",
    "\n",
    "    def save(self, save_folder):\n",
    "        np.save(f\"{save_folder}_state.npy\", self.state[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_action.npy\", self.action[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_next_state.npy\", self.next_state[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_reward.npy\", self.reward[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_not_done.npy\", self.not_done[:self.crt_size])\n",
    "        np.save(f\"{save_folder}_ptr.npy\", self.ptr)\n",
    "\n",
    "\n",
    "    def load(self, save_folder, size=-1):\n",
    "        reward_buffer = np.load(f\"{save_folder}_reward.npy\")\n",
    "\n",
    "        # Adjust crt_size if we're using a custom size\n",
    "        size = min(int(size), self.max_size) if size > 0 else self.max_size\n",
    "        self.crt_size = min(reward_buffer.shape[0], size)\n",
    "\n",
    "        self.state[:self.crt_size] = np.load(f\"{save_folder}_state.npy\")[:self.crt_size]\n",
    "        self.action[:self.crt_size] = np.load(f\"{save_folder}_action.npy\")[:self.crt_size]\n",
    "        self.next_state[:self.crt_size] = np.load(f\"{save_folder}_next_state.npy\")[:self.crt_size]\n",
    "        self.reward[:self.crt_size] = reward_buffer[:self.crt_size]\n",
    "        self.not_done[:self.crt_size] = np.load(f\"{save_folder}_not_done.npy\")[:self.crt_size]\n",
    "\n",
    "        print(f\"Replay Buffer loaded with {self.crt_size} elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1e6\n",
    "batch_size = 64\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim, batch_size, buffer_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CartPoleEnv' object has no attribute 'get_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-658168887ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# dataset = env.get_dataset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md4rl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqlearning_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/pytorch/d4rl/d4rl/__init__.py\u001b[0m in \u001b[0;36mqlearning_dataset\u001b[0;34m(env, dataset, terminate_on_end, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/gym/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CartPoleEnv' object has no attribute 'get_dataset'"
     ]
    }
   ],
   "source": [
    "# add dataset to the replay buffer\n",
    "\n",
    "# dataset = env.get_dataset()\n",
    "dataset = d4rl.qlearning_dataset(env)\n",
    "\n",
    "N = dataset['rewards'].shape[0]\n",
    "print('Loading buffer!')\n",
    "for i in range(N):\n",
    "    obs = dataset['observations'][i]\n",
    "    new_obs = dataset['observations'][i]\n",
    "    action = dataset['actions'][i]\n",
    "    reward = dataset['rewards'][i]\n",
    "    done_bool = bool(dataset['terminals'][i])\n",
    "    replay_buffer.add(obs, action, new_obs, reward, done_bool)\n",
    "print('Loaded buffer')\n",
    "replay_buffer.save('../buffers/{}'.format(env_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Buffer loaded with 1000000 elements.\n"
     ]
    }
   ],
   "source": [
    "replay_buffer.load('../buffers/{}'.format(env_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind:[628854 826410 810265 431175 332232 793047 230212 685823 899165 748961\n",
      " 495043  99054 631813 477215  18884 142498 404209 998740 904306 749428\n",
      " 125700 202121 712345 976871 177127 100216 593436 959705 219187  55585\n",
      " 411418 330868 325253 640326 722373 563043 359954 889642 201665 314726\n",
      " 283374 534941 709065 240131 419248 881084 516390 680063 754575 105060\n",
      " 453075 596235 338575 803498 284739 817068 671859 431516 555516 203580\n",
      " 562115 470280   8591  26205]\n"
     ]
    }
   ],
   "source": [
    "s1, s2, a ,r ,d = replay_buffer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n"
     ]
    }
   ],
   "source": [
    "print(s1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discrete_BCQ(object):\n",
    "    def __init__(self, action_dim, state_dim, device, discount=0.99, optimizer= \"Adam\", \n",
    "                 optimizer_parameters={}, target_update_frequency=8e3, initial_eps = 1, \n",
    "                 end_eps = 0.001, eps_decay_period = 25e4, eval_eps=0.001):\n",
    "        self.device = device\n",
    "        self.Q = DQN_fc(state_dim, action_dim)\n",
    "        self.Q_target = copy.deepcopy(self.Q)\n",
    "        self.Q_optimizer = getattr(torch.optim, optimizer)(self.Q.parameters(), **optimizer_parameters)\n",
    "        self.Q = self.Q.to(device)\n",
    "        self.Q_target = self.Q_target.to(device)\n",
    "\n",
    "        self.discount = discount\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "\n",
    "            # Decay for eps\n",
    "        self.initial_eps = initial_eps\n",
    "        self.end_eps = end_eps\n",
    "        self.slope = (self.end_eps - self.initial_eps) / eps_decay_period\n",
    "\n",
    "        # Evaluation hyper-parameters\n",
    "        self.state_shape = (-1, state_dim) ### need to pass framesize\n",
    "        self.eval_eps = eval_eps\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Number of training iterations\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def select_action(self, state, eval=False):\n",
    "        # eps for eval is fixed \n",
    "        if eval:\n",
    "            eps = self.eval_eps\n",
    "        else:\n",
    "            eps = max(self.slope * self.iterations + self.initial_eps, self.end_eps)\n",
    "            \n",
    "        # Select action according to policy with probability (1-eps)\n",
    "        # otherwise, select random action\n",
    "        if np.random.uniform(0, 1) > eps:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).reshape(state_shape).to(device)\n",
    "                return int(self.Q(state).argmax(1))\n",
    "        else:\n",
    "            return np.random.randint(self.num_actions)\n",
    "\n",
    "    def train(self, replay_buffer):\n",
    "        \n",
    "        # 1- Sample replay buffer\n",
    "        state, action, next_state, reward, done = replay_buffer.sample()\n",
    "\n",
    "        # 2- Compute the target Q value\n",
    "        with torch.no_grad():\n",
    "            target_Q = reward + done * self.discount * self.Q_target(next_state).max(1, keepdim=True)[0]\n",
    "\n",
    "        # Get current Q estimate\n",
    "        print(action[:5])\n",
    "        print('target_Q:{}'.format(target_Q[:5]))\n",
    "        current_Q = self.Q(state)\n",
    "\n",
    "        print('current_Q:{}'.format(current_Q[:5]))\n",
    "        current_Q = self.Q(state).gather(1, action)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Q loss\n",
    "        Q_loss = F.smooth_l1_loss(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the Q\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        Q_loss.backward()\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "        # Update target network by polyak or full copy every X iterations.\n",
    "        self.iterations += 1\n",
    "        if self.iterations % self.target_update_frequency == 0:\n",
    "            self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "        \n",
    "    def save(self, filename):\n",
    "        torch.save(self.Q.state_dict(), filename + \"_Q\")\n",
    "        torch.save(self.Q_optimizer.state_dict(), filename + \"_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.Q.load_state_dict(torch.load(filename + \"_Q\"))\n",
    "        self.Q_target = copy.deepcopy(self.Q)\n",
    "        self.Q_optimizer.load_state_dict(torch.load(filename + \"_optimizer\"))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 100)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametres\n",
    "eval_freq = 5e3\n",
    "max_timesteps = 1e6\n",
    "\n",
    "# agen parametrs\n",
    "discount = 0.99\n",
    "target_update_frequency = 1\n",
    "initial_eps = 0.1\n",
    "end_eps = 0.1\n",
    "eps_decay_period = 1\n",
    "eval_eps = 0\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  0],\n",
      "        [ 0, -1],\n",
      "        [-1,  1],\n",
      "        [ 0,  0],\n",
      "        [-1,  0]])\n",
      "target_Q:tensor([[ 0.0953],\n",
      "        [-0.0360],\n",
      "        [-0.0560],\n",
      "        [-0.0049],\n",
      "        [ 0.0390]])\n",
      "current_Q:tensor([[ 0.0962, -0.0080],\n",
      "        [-0.0736, -0.0363],\n",
      "        [-0.0566, -0.0971],\n",
      "        [-0.2861, -0.0050],\n",
      "        [ 0.0394, -0.0196]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-97871d27ffb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mtraining_iters\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-9ebcd2d0ff2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current_Q:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_Q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mcurrent_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 1 with size 2"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "episode_num = 0\n",
    "done = True\n",
    "training_iters = 0\n",
    "\n",
    "policy = discrete_BCQ(action_dim, state_dim, device, discount, \"Adam\", {\"lr\": 3e-4}, target_update_frequency,\n",
    "                      initial_eps, end_eps, eps_decay_period, eval_eps)\n",
    "    \n",
    "\n",
    "while training_iters < max_timesteps:\n",
    "    for _ in range(int(eval_freq)):\n",
    "        policy.train(replay_buffer)\n",
    "\n",
    "\n",
    "    evaluations.append(eval_policy(policy, env_name, seed))\n",
    "#     np.save(os.path.join(output_dir, f\"BCQ_{setting}\"), evaluations)\n",
    "\n",
    "    training_iters += int(eval_freq)\n",
    "    print(f\"Training iterations: {training_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
